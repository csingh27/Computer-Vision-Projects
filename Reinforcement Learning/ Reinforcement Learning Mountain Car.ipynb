{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "### Model-Free Learning\n",
    "#### Teaching a mountain car to reach the flag as fast as possible using Q-Learning\n",
    "# Theory\n",
    "- *What is Reinforcement Learning?*\n",
    "  Teaching a system to perform a task based on reward and punishment.\n",
    "- Example : \n",
    "1. Teaching a dog to obey orders.\n",
    "2. Agent : Dog\n",
    "3. State : Command\n",
    "4. Action : Laying down\n",
    "5. Policy : Rules that the agent uses to perform action\n",
    "- Why Reinforcement Learning?\n",
    "- *Disadvantages of Supervised Learning-*\n",
    "1. large data-set \n",
    "2. Imitates the actions of the human player (labelled data-sets). Agent can never be better than the human player.\n",
    "- Reinforcement Learning-\n",
    "*Do not have a data set*\n",
    "<img src=\"pic.svg\">\n",
    "- *Types of Learning - Model Based and Model Free*\n",
    "1. Q-Learning : Model free learning\n",
    "Model free Learning is when an AI can derive an optimal policy from its interactions with the environment\n",
    "without needing to create a model before hand.\n",
    "2. Q-Learning is a model-free learning technique that can be used to find the optimal action-selection policy using Q-Function\n",
    "\n",
    "Q-Table : Gives our state memory.Store values of each state-action combination (Q-values)\n",
    "<img src=\"qtable.png\">\n",
    "\n",
    "Q-values : Q-values represent the “quality” of an action taken from that state.\n",
    "<img src=\"formula.png\">\n",
    "Task : \n",
    "1. Agent- Car\n",
    "2. Actions- 3 : Drive Left, Do nothing, Drive Right\n",
    "3. Observation/ State- Position, Velocity (Continuous variables)\n",
    "\n",
    "## References \n",
    "Theory : \n",
    "AIDA Lecture 8\n",
    "https://www.youtube.com/watch?v=aCEvtRtNO-M\n",
    "Implementation\n",
    "AIDA Exercise 8\n",
    "https://www.novatec-gmbh.de/en/blog/introduction-to-q-learning/\n",
    "Exploration V exploitation : https://www.youtube.com/watch?v=mo96Nqlo1L8\n",
    "\n",
    "# Idea\n",
    "\n",
    "\n",
    "# Algorithm\n",
    "\n",
    "# Familiarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "[0.6  0.07]\n",
      "[-1.2  -0.07]\n",
      "Discrete(3)\n",
      "3\n",
      "[-0.58912799  0.        ]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make(\"MountainCar-v0\").env\n",
    "env.seed(0)\n",
    "print(env.observation_space) # 2 dimensions- position and velocity\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)\n",
    "print(env.action_space)\n",
    "print(env.action_space.n)\n",
    "print(env.reset())\n",
    "buckets_per_dimension=40\n",
    "q_table = np.zeros((buckets_per_dimension ** 2,3)) # Initialize q-table with zeroes\n",
    "print(q_table)\n",
    "last_state=None\n",
    "last_Action=None\n",
    "episode_length=1000\n",
    "actions_per_episode=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys #provides information in constant, functions and methods\n",
    "import gym #initialize the gym environment\n",
    "from gym import wrappers, logger\n",
    "#wrapper wraps multiple information (e.g. frames) from the env ;\n",
    "#Loggers contain basic functionality for diagnostics, hyperparameters, etc.\n",
    "from pprint import pprint #pretty printer\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent():\n",
    "    def __init__(self, action_space, observation_space):\n",
    "        self.action_space = action_space.n  # self.action_space will be a number n. You can return any integer x with 0 <= x <= n in act \n",
    "        self.observation_space = observation_space #x,y, v\n",
    "        self.buckets_per_dimension = 40  # Defines how many discrete bins are used in each dimesnion of the observation.\n",
    "        # Discrete steps are being made because out observation is continuous and we need discrete steps for markov process\n",
    "        self.q_table = np.zeros((self.buckets_per_dimension ** 2, self.action_space))\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def act(self, observation, last_reward, episode):\n",
    "        # Return 0 = push left, 1 = no push, 2 = push right\n",
    "        learning_rate = max(0.001, 1.0 * (0.85 ** int(episode/100)))\n",
    "        #reducing the learning rate as the no. of episodes increase but learning_rate>0.001 \n",
    "        #exploring in the beginning (take a random sample of the action space)= 1.0 epsilon greedy\n",
    "        #and exploiting in the end (determine next action)\n",
    "        # 0.85 is the decay multiplied by epsilon after each episode\n",
    "        lookahead = 1        \n",
    "        state = self.to_state(observation)\n",
    "        if self.last_state is not None:\n",
    "            self.q_table[self.last_state][self.last_action] = self.q_table[self.last_state][self.last_action] \\\n",
    "                + learning_rate * (last_reward + lookahead * np.max(self.q_table[state]) - self.q_table[self.last_state][self.last_action])\n",
    "        possible_actions = self.q_table[state]\n",
    "        probabilities = np.exp(possible_actions) / np.sum(np.exp(possible_actions))\n",
    "        choice = np.random.choice(self.action_space, p=probabilities)\n",
    "        self.last_state = state\n",
    "        self.last_action = choice\n",
    "        return choice\n",
    "\n",
    "\n",
    "    def to_state(self, observation):\n",
    "        upper_bound_position = self.observation_space.high[0]\n",
    "        lower_bound_position = self.observation_space.low[0]\n",
    "        upper_bound_velocity = self.observation_space.high[1]\n",
    "        lower_bound_velocity = self.observation_space.low[1]\n",
    "        step_size_position = (upper_bound_position - lower_bound_position) / self.buckets_per_dimension\n",
    "        step_size_velocity = (upper_bound_velocity - lower_bound_velocity) / self.buckets_per_dimension\n",
    "        bucket_position = int((observation[0] - lower_bound_position) / step_size_position)\n",
    "        bucket_velocity = int((observation[1] - lower_bound_velocity) / step_size_velocity)\n",
    "        return bucket_position * self.buckets_per_dimension + bucket_velocity\n",
    "\n",
    "    \n",
    "    def get_best_actions(self):\n",
    "        return np.argmax(self.q_table, axis=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You can set the level to logger.DEBUG or logger.WARN if you\n",
    "    # want to change the amount of output.\n",
    "    logger.set_level(logger.INFO)\n",
    "\n",
    "    env = gym.make(\"MountainCar-v0\").env\n",
    "\n",
    "    env.seed(0)\n",
    "    agent = QLearningAgent(env.action_space, env.observation_space)\n",
    "\n",
    "    episode_count = 1000\n",
    "    attempts_in_episode = 10000\n",
    "\n",
    "    for ep in range(episode_count):\n",
    "        ob = env.reset()\n",
    "        reward = 0\n",
    "        action = None\n",
    "        done = False\n",
    "        for j in range(attempts_in_episode):\n",
    "            action = agent.act(ob, reward, ep)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "             env.render()\n",
    "            if done:\n",
    "                print(\"Flag reached!\")\n",
    "                break\n",
    "    \n",
    "    input(\"Press Enter to continue and show best solution...\")\n",
    "    ob = env.reset()    \n",
    "    best = agent.get_best_actions()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = best[agent.to_state(ob)]\n",
    "        ob, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "    print(\"Done\") \n",
    "\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
